# Lecture 4  
## 딥러닝 모형의 구조와 학습  

---

### 1️⃣ 딥러닝 기본 구조  

- 입력층 → 은닉층 → 출력층  
- 각 노드는 가중치와 편향을 가지며 활성화함수로 출력 생성  
- 학습: 손실함수를 최소화하도록 가중치 갱신 (오차역전파 사용)

---

### 2️⃣ 활성화 함수  

| 함수 | 식 | 특징 |
|------|----|------|
| Sigmoid | \( \frac{1}{1+e^{-x}} \) | 확률형 출력, 경사소실 |
| tanh | \( \frac{e^x - e^{-x}}{e^x + e^{-x}} \) | 범위(-1~1) |
| ReLU | \( \max(0,x) \) | 빠름, 경사소실 완화 |
| Leaky ReLU | \( \max(0.01x,x) \) | 음수영역도 반영 |
| ELU | \( x>0:x, \ x≤0:\alpha(e^x-1) \) | 안정적, 연속적 |

> 층별 활성화함수가 모두 선형이면 신경망 전체도 선형이 되어 층을 깊게 쌓는 의미가 사라짐.

---

### 3️⃣ 오차역전파 (Backpropagation)

- 순전파: 입력 → 출력 → 손실함수 계산  
- 역전파: 손실함수를 미분해 가중치를 역방향으로 갱신  

$$
w := w - \eta \frac{\partial J(w)}{\partial w}
$$

- \( \eta \): 학습률  
- 경사하강법 기반 학습

---

### 4️⃣ 최적화 알고리즘  

| 방법 | 설명 |
|------|------|
| GD | 전체 데이터 사용, 안정적 but 느림 |
| SGD | 1개 샘플, 빠르지만 불안정 |
| Mini-Batch | 일부 샘플 묶음, 일반적 |
| Momentum | 이전 기울기 반영해 진동 억제 |
| AdaGrad | 변수별 학습률 조정, 후반 느림 |
| RMSProp | AdaGrad 개선, 지수평균 |
| **Adam** | Momentum + RMSProp 결합 |

#### Adam 공식  
$$
m_t = \beta_1 m_{t-1} + (1-\beta_1)g_t \\
s_t = \beta_2 s_{t-1} + (1-\beta_2)g_t^2 \\
\hat{m}_t = \frac{m_t}{1-\beta_1^t}, \quad \hat{s}_t = \frac{s_t}{1-\beta_2^t} \\
w := w - \eta \frac{\hat{m}_t}{\sqrt{\hat{s}_t}+\epsilon}
$$

---

### 5️⃣ 일반근사정리 (Universal Approximation Theorem)
> “충분한 뉴런을 가진 은닉층 하나로 모든 연속함수를 근사 가능”

- 단층만으로도 가능하지만 비효율  
- 층이 깊을수록 표현력 ↑, 추상화 ↑

---

### 6️⃣ 데이터 분할  

| 데이터 | 역할 |
|--------|------|
| **훈련** | 가중치 학습, 손실 최소화 |
| **검증** | 일반화 성능 점검, 모델 선택 |
| **시험** | 최종 성능 평가 |

---

### 7️⃣ 과대적합 방지  

| 방법 | 설명 |
|------|------|
| 조기종료 | 검증 손실 증가 시 학습 중단 |
| 정칙화 | 손실함수에 벌칙항 추가 |
| 드롭아웃 | 일부 뉴런 연결 제거 |
| 데이터증강 | 회전, 반전, 노이즈, 확대 |
| 배치정규화 | 입력값 표준화로 학습 안정화 |

---

### 8️⃣ 제한된 볼츠만 머신 (RBM) & 심층신뢰망 (DBN)

- **RBM**: 출력층 없이 입력–은닉층 왕복 학습  
- **DBN**: 여러 RBM을 적층해 출력층 연결  
  - 각 층을 사전학습(pre-training) 후 미세조정(fine-tuning)  
  - 초기값 문제 해결

---

### 9️⃣ 합성곱신경망 (CNN)

- 이미지 인식에 특화된 구조  
- **합성곱(Convolution)**  
  - 필터(kernel)로 특징 추출  
  - 패딩(Padding): 크기 보정  
  - 스트라이드(Stride): 이동 간격  
- **풀링(Pooling)**  
  - Max/Average Pooling으로 차원 축소  

---

### 🔟 CNN 발전

| 모델 | 특징 |
|------|------|
| AlexNet | ReLU + 드롭아웃 + GPU |
| VGGNet | 3×3 필터 반복 |
| GoogLeNet | 인셉션 모듈(가중치↓, 연산↓) |
| ResNet | 잔차학습 + 스킵연결로 경사소실 해결 |

---

### 11️⃣ 기타 주요 개념

- **지역반응정규화 (LRN)**: 특정 뉴런 과활성화 시 주변값으로 정규화  
- **인셉션(Inception)**: 신경망 속 신경망 구조, 여러 필터 병렬 적용  
- **컬러 합성곱**: RGB 각 채널별로 필터 적용 후 합산  

---

### 12️⃣ 객체 탐지 모델  

| 모델 | 설명 |
|------|------|
| **R-CNN 계열** | 후보영역 → CNN → 분류 (단계적) |
| **YOLO** | 전체 이미지를 한 번에 처리, 실시간 속도 |

---

> **요약:**  
> 딥러닝 학습은 오차역전파로 손실을 최소화하고,  
> 정규화·드롭아웃·데이터증강으로 과적합을 방지하며,  
> CNN·Inception·ResNet 등으로 구조적 성능을 향상시킨다.