## Lecture 0

 - 인공지능 > 머신러닝 > 딥러닝<br>
 - 인공신경망 - 실제 인간의 신경망에서 영감얻어 출력을 추정, 입력값의 중요도에따라 가중치부여

 - 심층신경망 - 입력 - 은닉 - 출력 계층으로 나뉘며 각 노드가 가중치를 의미함

 - 딥러닝은 계층적 학습 - 층별로 학습에서 역할이 다름(기초~추상적인 부분)

 - 딥러닝은 기출을 보고 답예측하는거처럼 예시 많이보고 학습해 다음에 뭐가나올지 확률로 예상하는 기계


 ## Lecture 1
### 머신러닝 : 규칙이 아닌 컴퓨터가 학습해 패턴을 찾아내는 방법
- 답이있는 데이터를 넣어주면 데이터에서 패턴을 찾아내 학습함(스스로!)
- 구성요소는 과제, 데이터, 모형, 손실함수, 최적화 알고리즘
### 머신러닝 구성
1) 과제 - 발병여부나 소득예측 등
2) 데이터 - 숫자, 이미지, 텍스트 / 입력,출력데이터로 나뉨 -> 지도학습, 비지도학습(출력데이터X)으로 나뉨
3) 모형 - 확률모형(회귀), 알고리즘(딥러닝), 모수를 포함한 함수
4) 손실함수 : 오차의 정도를 나타냄 
5) 최적화 알고리즘 : 손실함수를 최소로하는 모형의 모수를 찾는 알고리즘

### 머신러닝 구성
 - 초깃값 주고 경사하강법 같은 손실함수를 최소로하는 최적값 찾기 -> 이후 모형이 만들어지면 최적의 모형에 데이터 적용해 점검 -> 손실함수 줄어들도록 지속적으로 수정, 학습



#### 학습방법
 - 지도, 비지도, 준지도, 자기지도, 강화
 1) 지도학습 : 입력데이터 x로 출력데이터 y를 추정(기출문제 풀고 답보면서 학습) 회귀모형
 2) 비지도 : only입력데이터, 결과도출이 아닌 확률분포 학습(답을 모르는 시험에서 문제 안읽고 선지분석해서 답찾기)
 3) 준지도 : 지도, 비지도 섞여있을때
 4) 자기지도 : 레이블 없고 only 입력데이터 (비지도의 일종) 레이블 생성한후 학습, BERT, GPT등
 5) 강화학습 : 환경<->행동 상호작용에서 발생되는 데이터사용해 학습 누적보상액이 최대가 되게하고, 시행착오 통해 목표도달

### 딥러닝
 - CNN : 이미지 인식, 식별에 사용됨 뉴런전체가 활성화 되는게아닌 필터(kernel)이용하고 MLP적용한 모형
 - 오토인코더 : 입력층=출력층 으로 설계
 
- 신경망 작성 : 머신러닝과 동일, 데이터(train, test)로 구분 -> 신경망을 경사하강법으로 학습 -> 결과와 출력데이터 차이로 손실함수 계산 -> 초기값주고 역전파사용해 손실함수 최소화

- 인공신경망의 한계 : 깊이 깊어지면 정확도 향상 but 학습어려워짐

## Lecture 2
 - 딥러닝 모형은 과정을 알 수 없고 결과의 예측만 목적 so 사건의 원인을 제대로 설명하지못함
 - 멀티누이 분포 : MNIST같은 숫자를 0~9로 예측하는 과제, 원핫인코딩
 - 퍼셉트론 : 신경망으로 이미지인식, 선형 이진분류기, 앞의수식을 학습해 오차를 이용해 가중치 갱신, 손실함수는 정하지않음
 - 아달린 : 퍼셉트론의 문제(손실함수)해결 선형 활성화함수 사용 sigma(z)=z / 가중치갱신방법이 다름
 - 최소제곱법 : 손실함수(입력-출력)의 제곱합
 
 | 항목         | 퍼셉트론                                   | ADALINE                                         |
|--------------|-------------------------------------------|------------------------------------------------|
| 출력         | 계단함수(0/1 또는 ±1)                      | 선형(연속값)                                    |
| 손실         | 퍼셉트론 기준(오분류에만 반응)              | MSE(모든 오차에 연속적으로 반응)                 |
| 업데이트     | 오분류 시에만 가중치 변경                   | 오차 크기 비례로 항상 미세 조정                  |
| 수학적 성격  | 비미분 가능 목표, 수렴 정리는 “선형분리 시” | 미분 가능 목표, 선형회귀와 동일한 해(Wiener 해) 지향 |
| 실무 감각    | 경계만 맞추려 함                          | 경계 및 마진/잔차까지 고려 → 더 안정적            |

### 선형회귀모형
#### 설명변수로 종속변수 예측 (MSE, MLE)
 - MSE 
 #### 최소제곱법

목적함수:
$$
J(w) = \frac{1}{2n} \sum_{i=1}^{n} (y_i - w_0 - w_1 x_i)^2
$$

---

$J(w)$ 를 최소화하기 위해 $w_0, w_1$ 에 대해 각각 미분:

정규방정식:
$$
\frac{\partial J(w)}{\partial w_0}
= - \sum_{i=1}^{n} (y_i - w_0 - w_1 x_i) \equiv 0
$$

$$
\frac{\partial J(w)}{\partial w_1}
= - \sum_{i=1}^{n} (y_i - w_0 - w_1 x_i)x_i \equiv 0
$$

---

추정량:
$$
\hat{w}_0 = \bar{y} - \hat{w}_1 \bar{x}, 
\quad
\hat{w}_1 = \frac{\sum (x_i - \bar{x})(y_i - \bar{y})}{\sum (x_i - \bar{x})^2}
$$


- MLE
#### 최대가능도추정법

가능도함수:
$$
L(w) = \prod_{i=1}^{n} \frac{1}{\sqrt{2\pi\sigma^2}}
\exp\left[ -\frac{(y_i - w_0 - w_1 x_i)^2}{2\sigma^2} \right]
$$

정리하면:
$$
L(w) = \left( \frac{1}{\sqrt{2\pi\sigma^2}} \right)^n
\exp\left[ -\frac{1}{2\sigma^2} \sum_{i=1}^{n} (y_i - w_0 - w_1 x_i)^2 \right]
$$

---

- 가능도함수를 최대화하는 $w_0, w_1$ 을 구하게 됨  
- 로그가능도함수는  
$$
\log L(w) \propto -\frac{1}{2} \sum_{i=1}^{n} (y_i - w_0 - w_1 x_i)^2
$$

- 따라서 $J(w)$ 를 최소화하는 $w_0, w_1$ 을 구하게 됨  
- **⇒ 최소제곱법에 의한 결과와 같음**


#### 신경망표현


#### Newton's method
 -  g(x) = 0을 찾는것 계속 미분하면서 값찾음

#### 경사하강법
 - 경사줄이며 가중치 갱신, 시작지점의 반대부호로 움직이면 최저점나옴, 초기값은 0이 default, global과 local을 고려해야함, 학습률(이동량)에 따라 수렴속도 달라짐, 학습률을 적절히 조절해야 튀거나 local에 머무르지않고 조정가능<br>
 그 방법중 하나는 처음에 크게, 다음에 작게해서 조정
- stochastic gradient decent : 랜덤으로 1개 선택해 기울기 구하고 가중치 갱신, random이라 계산빠르고 local에 머무르지 않음 but 학습시간 커짐
- mibi batch : 일부데이터 사용 20~30개정도로 잘라서, 시간짧

* epoch / batch : 전체학습횟수 / 훈련데이터 나눈개수


### 로지스틱회귀, 소프트맥스회귀

#### 로지스틱 회귀
 - 출력값 : 이진분류
 - 신경망 : 
 - 활성화 함수 : 시그모이드 함수
$$
g(x) = \frac{1}{1 + e^{-x}}
$$
---
 - 실제 분류값: $0$ 과 $1$  



 - 출력값: 선형결합된 값을 시그모이드 함수를 통해 나오는 $0$ 과 $1$ 사이의 확률값  

$$
\pi(x) = g\left( \sum_{j=0}^p w_j x_j \right) = g(xw)
= \frac{1}{1 + \exp\left(-\sum_{j=0}^p w_j x_j \right)}
$$

---

조건부 확률:
$$
\pi(x) = P(y = 1 \mid x)
$$

예측값:
$$
\hat{y} =
\begin{cases}
1, & \pi(x) \geq 0.5 \\
0, & \pi(x) < 0.5
\end{cases}
$$

---

모형 가정:  
$$
y_i \sim \text{Bernoulli}(\pi_i), \quad \pi_i = \pi(x_i)
$$

조건부 확률:
$$
p(y_i \mid x_i, w) = \pi_i(x)^{y_i} (1 - \pi_i(x))^{1 - y_i}, 
\quad y_i \in \{0,1\}
$$

---

가능도 함수:
$$
L(w) = \prod_{i=1}^n \pi_i^{y_i} (1 - \pi_i)^{1 - y_i}
$$
### 로그 가능도 함수

$$
\log L(w) = \sum_{i=1}^n \left[ y_i \log(\pi_i) + (1 - y_i) \log(1 - \pi_i) \right]
$$

로지스틱 회귀모형은 미분을 통해 $w$를 직접 구할 수 없기 때문에  
경사하강법으로 구해야 함.

---

### 손실함수

$$
J(w) = - \log L(w) = \sum_{i=1}^n J_i(w)
$$

$$
J_i(w) = - \Big[ y_i \log(\pi_i) + (1 - y_i) \log(1 - \pi_i) \Big]
$$

---

### 시그모이드 함수

$$
\pi_i = \frac{1}{1 + \exp\left(- \sum_{l=0}^p w_l x_{il}\right)}
$$

---

### 편미분 과정

$$
\frac{\partial \pi_i}{\partial w_j}
= - \left[ \frac{1}{1 + \exp\left(- \sum_{l=0}^p w_l x_{il}\right)} \right]^2
\exp\left( - \sum_{l=0}^p w_l x_{il} \right)(-x_{ij})
= \pi_i (1 - \pi_i) x_{ij}
$$

---

### 그래디언트

$$
\frac{\partial J(w)}{\partial w_j}
= \sum_{i=1}^n \frac{\partial J_i(w)}{\partial \pi_i} \cdot \frac{\partial \pi_i}{\partial w_j}
= - \sum_{i=1}^n \left[ \frac{y_i}{\pi_i} - \frac{1 - y_i}{1 - \pi_i} \right]
\pi_i (1 - \pi_i) x_{ij}
$$

$$
\frac{\partial J(w)}{\partial w_j}
= - \sum_{i=1}^n (y_i - \pi_i) x_{j}
$$
### 경사하강법 업데이트

$$
w_j := w_j - \eta \frac{\partial J(w)}{\partial w_j}
$$

$$
w_j = w_j + \eta \sum_{i=1}^n (y_i - \pi_i) x_{ij}
$$

---

### 소프트맥스(Softmax) 회귀모형

- **다중범주 분류**
  - 예: MNIST 손글씨 숫자 (0, 1, 2, …, 9) 의 10개 그룹 분류  
  - 예: 붓꽃 데이터에서 붓꽃의 3종 분류  

- 로지스틱 회귀모형의 일반화  

---

#### 확률 표현: 소프트맥스 함수
$$
g(x_k) = \frac{\exp(x_k)}{\sum_{l} \exp(x_l)}
$$

#### 출력변수
$$
y_i \in \{1, 2, \ldots, K\}, 
\quad \pi_i = (\pi_i^1, \pi_i^2, \ldots, \pi_i^K),
\quad \sum_{k=1}^K \pi_i^k = 1
$$

#### 다항분포 가정
$$
y_i \sim \text{Multinoulli}(\pi_i^1, \pi_i^2, \ldots, \pi_i^K)
$$
### 소프트맥스 회귀 (Softmax Regression)

#### 클래스 $k$에 속할 확률
$$
\pi_i^k = P(y_i = k \mid x_i; w^k)
= \frac{\exp(x_i w^k)}{\sum_{l=1}^K \exp(x_i w^l)}
$$

---

#### 가중치 $w_j^k$에 대한 편미분
$$
\frac{\partial \pi_i^k}{\partial w_j^k} =
\begin{cases}
\pi_i^j (1 - \pi_i^j) x_{ij}, & k = j \\
- \pi_i^k \pi_i^j x_{ij}, & k \neq j
\end{cases}
$$

---

#### 손실함수 (= 교차엔트로피)
$$
J(w) = - \sum_i \sum_k y_i^k \log(\pi_i^k),
\qquad
y_i^k =
\begin{cases}
1, & y_i = k \\
0, & y_i \neq k
\end{cases}
$$

---

#### 그래디언트
$$
\frac{\partial J(w)}{\partial w^k}
= - \sum_i (y_i^k - \pi_i^k) x_i
$$
### 소프트맥스 회귀 - 추가 정리

#### 확률
$$
\pi_i^k = \frac{\exp(x w^k)}{\sum_{l=1}^K \exp(x w^l)}
$$

#### 편미분
$$
\frac{\partial J(w)}{\partial w_0^k} = - \sum_i (y_i^k - \pi_i^k)
$$

#### 가중치 갱신 (경사하강법)
$$
w_j^k := w_j^k - \eta \frac{\partial J(w)}{\partial w_j^k}
$$

---

### 출력변수 / 활성화함수 / 손실함수 요약표

| 출력변수 | 분포          | 활성화함수  | 손실함수        |
|----------|---------------|-------------|----------------|
| 이산형   | 베르누이분포  | 시그모이드  | 이진 엔트로피   |
| 이산형   | 멀티누이분포  | 소프트맥스  | 교차 엔트로피   |
| 연속형   | 정규분포      | 선형        | 평균제곱오차    |
